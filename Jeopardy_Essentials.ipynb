{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe86e97",
   "metadata": {},
   "source": [
    "I'm starting off by cpying the code from my Basics Badge Project to setup a DataFrame from the .json file... before moving on to trying out other vecorization and/or prediciton strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6fbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "#import os\n",
    "#import re\n",
    "from string import punctuation\n",
    "#from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62765e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cman0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cman0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cman0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\cman0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# may need to download stopwords from nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726a4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_stopwords += punctuation\n",
    "\n",
    "# create function to clean the question bank\n",
    "def clean_wordlist(questions):\n",
    "    '''\n",
    "    Takes a list of questions and cleans them into a tokenized list\n",
    "    of lemmatized words.\n",
    "    '''\n",
    "    corpus = ''\n",
    "    wordlist = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for q in questions:\n",
    "        corpus += q.lower()\n",
    "    wordlist += [lemmatizer.lemmatize(word) for word in\n",
    "                word_tokenize(corpus) if word not in english_stopwords]\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3051a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df from the .json file\n",
    "df = pd.read_json('jeopardy.json')\n",
    "\n",
    "# turn $values into int's\n",
    "df['value'] = df['value'].str.replace('[^0-9]', '', regex=True)\n",
    "df['value'] = df['value'].fillna(0)\n",
    "df['value'] = df['value'].astype('int')\n",
    "\n",
    "# create new collumn in df containing binary values for high vs low\n",
    "df['high_low'] = [1 if x>600 else 0 for x in df['value']]\n",
    "\n",
    "# create new collumn in df containing cleaned questions.\n",
    "\n",
    "# NO LONGER NEEDED\n",
    "#df['q_words'] = [' '.join(clean_wordlist([x])) for x in df['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf942309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>air_date</th>\n",
       "      <th>question</th>\n",
       "      <th>value</th>\n",
       "      <th>answer</th>\n",
       "      <th>round</th>\n",
       "      <th>show_number</th>\n",
       "      <th>high_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTORY</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'For the last 8 years of his life, Galileo was...</td>\n",
       "      <td>200</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'No. 2: 1912 Olympian; football star at Carlis...</td>\n",
       "      <td>200</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'The city of Yuma in this state has a record a...</td>\n",
       "      <td>200</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'In 1963, live on \"The Art Linkletter Show\", t...</td>\n",
       "      <td>200</td>\n",
       "      <td>McDonald\\'s</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'Signer of the Dec. of Indep., framer of the C...</td>\n",
       "      <td>200</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216925</th>\n",
       "      <td>RIDDLE ME THIS</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>'This Puccini opera turns on the solution to 3...</td>\n",
       "      <td>2000</td>\n",
       "      <td>Turandot</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>4999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216926</th>\n",
       "      <td>\"T\" BIRDS</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>'In North America this term is properly applie...</td>\n",
       "      <td>2000</td>\n",
       "      <td>a titmouse</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>4999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216927</th>\n",
       "      <td>AUTHORS IN THEIR YOUTH</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>'In Penny Lane, where this \"Hellraiser\" grew u...</td>\n",
       "      <td>2000</td>\n",
       "      <td>Clive Barker</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>4999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216928</th>\n",
       "      <td>QUOTATIONS</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>'From Ft. Sill, Okla. he made the plea, Arizon...</td>\n",
       "      <td>2000</td>\n",
       "      <td>Geronimo</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>4999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216929</th>\n",
       "      <td>HISTORIC NAMES</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>'A silent movie title includes the last name o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Grigori Alexandrovich Potemkin</td>\n",
       "      <td>Final Jeopardy!</td>\n",
       "      <td>4999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216930 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               category    air_date  \\\n",
       "0                               HISTORY  2004-12-31   \n",
       "1       ESPN's TOP 10 ALL-TIME ATHLETES  2004-12-31   \n",
       "2           EVERYBODY TALKS ABOUT IT...  2004-12-31   \n",
       "3                      THE COMPANY LINE  2004-12-31   \n",
       "4                   EPITAPHS & TRIBUTES  2004-12-31   \n",
       "...                                 ...         ...   \n",
       "216925                   RIDDLE ME THIS  2006-05-11   \n",
       "216926                        \"T\" BIRDS  2006-05-11   \n",
       "216927           AUTHORS IN THEIR YOUTH  2006-05-11   \n",
       "216928                       QUOTATIONS  2006-05-11   \n",
       "216929                   HISTORIC NAMES  2006-05-11   \n",
       "\n",
       "                                                 question  value  \\\n",
       "0       'For the last 8 years of his life, Galileo was...    200   \n",
       "1       'No. 2: 1912 Olympian; football star at Carlis...    200   \n",
       "2       'The city of Yuma in this state has a record a...    200   \n",
       "3       'In 1963, live on \"The Art Linkletter Show\", t...    200   \n",
       "4       'Signer of the Dec. of Indep., framer of the C...    200   \n",
       "...                                                   ...    ...   \n",
       "216925  'This Puccini opera turns on the solution to 3...   2000   \n",
       "216926  'In North America this term is properly applie...   2000   \n",
       "216927  'In Penny Lane, where this \"Hellraiser\" grew u...   2000   \n",
       "216928  'From Ft. Sill, Okla. he made the plea, Arizon...   2000   \n",
       "216929  'A silent movie title includes the last name o...      0   \n",
       "\n",
       "                                answer             round  show_number  \\\n",
       "0                           Copernicus         Jeopardy!         4680   \n",
       "1                           Jim Thorpe         Jeopardy!         4680   \n",
       "2                              Arizona         Jeopardy!         4680   \n",
       "3                          McDonald\\'s         Jeopardy!         4680   \n",
       "4                           John Adams         Jeopardy!         4680   \n",
       "...                                ...               ...          ...   \n",
       "216925                        Turandot  Double Jeopardy!         4999   \n",
       "216926                      a titmouse  Double Jeopardy!         4999   \n",
       "216927                    Clive Barker  Double Jeopardy!         4999   \n",
       "216928                        Geronimo  Double Jeopardy!         4999   \n",
       "216929  Grigori Alexandrovich Potemkin   Final Jeopardy!         4999   \n",
       "\n",
       "        high_low  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "216925         1  \n",
       "216926         1  \n",
       "216927         1  \n",
       "216928         1  \n",
       "216929         0  \n",
       "\n",
       "[216930 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e3a80",
   "metadata": {},
   "source": [
    "Ok, back to where we were on the Basics Badge right before training our model using tfidf and naive bayes. This time I plan to use word embeddings from a pre-trained word2vec model. I guess the first thing to do is download the the GoogleNews vectors we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2d09ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-13 19:09:41--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.139.109\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.139.109|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G   170KB/s    in 2h 56m  \n",
      "\n",
      "2022-02-13 22:06:34 (152 KB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download GoogleNews vectors file if needed\n",
    "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4f684",
   "metadata": {},
   "source": [
    "That took a while.... ok now to grab the keyed vectors from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98db1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d21a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = './GoogleNews-vectors-negative300.bin'\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(embeddings_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b7e6e",
   "metadata": {},
   "source": [
    "I tried making this next part a function, but... it didn't work out. Just gonna do it like this and comment along the way.\n",
    "\n",
    "Basically running each word (of each question) through Google's word embeddings to get their word vector arrays, and then getting the averaging (mean) of the word vector arrays for each question, which gives us a \"question vector\" array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de1494a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# googled a way to stop the \"FutureWarning\" message from coming up for every itteration\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#hope it works because that was really annoying\n",
    "\n",
    "question_vectors = pd.DataFrame()\n",
    "    # creating a string of lowercase words for each questuion and \n",
    "    # itterating through them\n",
    "for question in df['question'].str.lower().str.replace('[^a-z ]',''):\n",
    "    # creating a temporary datafame to hold all of the word vectors\n",
    "    temp = pd.DataFrame()\n",
    "    # itterating through each word\n",
    "    for word in question.split(' '):\n",
    "        # skip stop words\n",
    "        if not word in english_stopwords:\n",
    "                   # skip words not in Google's word embeddings\n",
    "            try:   # grab the vector for the word\n",
    "                word_vec = word_vectors[word]\n",
    "                # put it in the temp dataframe\n",
    "                temp = temp.append(pd.Series(word_vec), ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "        # create vector for this question using the average\n",
    "        # of the vectors of all the words in the question\n",
    "    this_q_vec = temp.mean()\n",
    "        # put the question's vector into the main DataFrame\n",
    "    question_vectors = question_vectors.append(this_q_vec, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c5d77",
   "metadata": {},
   "source": [
    "Got lots of warnings there!\n",
    "FutureWarning? ...sounds like a problem for another day. :D\n",
    "Guess I'll eventuall have to learn how to use pandas.concat instead of the frame.append method.\n",
    "\n",
    "But today's problem is that the VM I'm on can't handle this. I guess we've outgrown this. Time to upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "872e41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216930, 300)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053513</td>\n",
       "      <td>0.028366</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0.060440</td>\n",
       "      <td>-0.011749</td>\n",
       "      <td>0.018772</td>\n",
       "      <td>0.052729</td>\n",
       "      <td>-0.096954</td>\n",
       "      <td>0.073669</td>\n",
       "      <td>0.170628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116028</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>-0.076401</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>-0.106514</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.023376</td>\n",
       "      <td>-0.097069</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.033997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.070263</td>\n",
       "      <td>0.130393</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.083141</td>\n",
       "      <td>0.067061</td>\n",
       "      <td>-0.009216</td>\n",
       "      <td>-0.016324</td>\n",
       "      <td>-0.120292</td>\n",
       "      <td>-0.052221</td>\n",
       "      <td>0.085904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.054294</td>\n",
       "      <td>-0.065976</td>\n",
       "      <td>-0.066806</td>\n",
       "      <td>0.032055</td>\n",
       "      <td>-0.100475</td>\n",
       "      <td>-0.066428</td>\n",
       "      <td>-0.057850</td>\n",
       "      <td>0.082839</td>\n",
       "      <td>0.022688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009086</td>\n",
       "      <td>0.069615</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.038376</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>-0.093994</td>\n",
       "      <td>-0.009918</td>\n",
       "      <td>-0.081107</td>\n",
       "      <td>0.069406</td>\n",
       "      <td>0.085458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020455</td>\n",
       "      <td>-0.007708</td>\n",
       "      <td>-0.079356</td>\n",
       "      <td>-0.022810</td>\n",
       "      <td>0.028102</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>-0.089992</td>\n",
       "      <td>-0.023980</td>\n",
       "      <td>0.044006</td>\n",
       "      <td>-0.089634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037720</td>\n",
       "      <td>-0.066895</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>0.121809</td>\n",
       "      <td>-0.046596</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.122977</td>\n",
       "      <td>-0.070626</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.102086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056553</td>\n",
       "      <td>-0.096819</td>\n",
       "      <td>-0.239641</td>\n",
       "      <td>0.132499</td>\n",
       "      <td>0.092111</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>-0.082136</td>\n",
       "      <td>-0.019723</td>\n",
       "      <td>-0.042411</td>\n",
       "      <td>-0.167899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068912</td>\n",
       "      <td>-0.043349</td>\n",
       "      <td>0.059557</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>-0.041477</td>\n",
       "      <td>-0.008450</td>\n",
       "      <td>-0.088745</td>\n",
       "      <td>0.126736</td>\n",
       "      <td>0.021566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074632</td>\n",
       "      <td>-0.128906</td>\n",
       "      <td>-0.114217</td>\n",
       "      <td>0.062384</td>\n",
       "      <td>-0.047323</td>\n",
       "      <td>-0.034776</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>0.143745</td>\n",
       "      <td>0.067464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216925</th>\n",
       "      <td>0.107361</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>-0.014933</td>\n",
       "      <td>0.108297</td>\n",
       "      <td>-0.053955</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.122599</td>\n",
       "      <td>0.017049</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.141591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253113</td>\n",
       "      <td>0.040649</td>\n",
       "      <td>-0.240438</td>\n",
       "      <td>-0.003825</td>\n",
       "      <td>-0.082634</td>\n",
       "      <td>0.094910</td>\n",
       "      <td>-0.000905</td>\n",
       "      <td>0.041829</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.024526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216926</th>\n",
       "      <td>-0.091732</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>0.088677</td>\n",
       "      <td>-0.008965</td>\n",
       "      <td>-0.138165</td>\n",
       "      <td>-0.011814</td>\n",
       "      <td>-0.078559</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.077260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054901</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>-0.077101</td>\n",
       "      <td>0.111030</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>-0.004869</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>-0.007151</td>\n",
       "      <td>0.124410</td>\n",
       "      <td>-0.069255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216927</th>\n",
       "      <td>0.017008</td>\n",
       "      <td>0.138156</td>\n",
       "      <td>0.094830</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>0.093838</td>\n",
       "      <td>-0.060109</td>\n",
       "      <td>-0.115553</td>\n",
       "      <td>0.040952</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038896</td>\n",
       "      <td>-0.038656</td>\n",
       "      <td>-0.206906</td>\n",
       "      <td>0.078179</td>\n",
       "      <td>-0.065199</td>\n",
       "      <td>-0.045064</td>\n",
       "      <td>-0.102254</td>\n",
       "      <td>0.066569</td>\n",
       "      <td>0.072320</td>\n",
       "      <td>0.055420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216928</th>\n",
       "      <td>-0.009044</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>0.027116</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>-0.017256</td>\n",
       "      <td>-0.122869</td>\n",
       "      <td>0.044478</td>\n",
       "      <td>-0.030525</td>\n",
       "      <td>0.089511</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144798</td>\n",
       "      <td>0.059027</td>\n",
       "      <td>-0.053511</td>\n",
       "      <td>-0.043599</td>\n",
       "      <td>0.018205</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>-0.062600</td>\n",
       "      <td>-0.144731</td>\n",
       "      <td>0.088224</td>\n",
       "      <td>-0.081587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216929</th>\n",
       "      <td>0.081014</td>\n",
       "      <td>0.060028</td>\n",
       "      <td>0.055583</td>\n",
       "      <td>0.102417</td>\n",
       "      <td>-0.012332</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.021077</td>\n",
       "      <td>-0.052429</td>\n",
       "      <td>0.106099</td>\n",
       "      <td>0.080048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042480</td>\n",
       "      <td>-0.012171</td>\n",
       "      <td>-0.113688</td>\n",
       "      <td>-0.022196</td>\n",
       "      <td>0.027520</td>\n",
       "      <td>-0.100634</td>\n",
       "      <td>-0.027883</td>\n",
       "      <td>-0.065681</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>0.075165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216930 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0       0.053513  0.028366 -0.003403  0.060440 -0.011749  0.018772  0.052729   \n",
       "1      -0.070263  0.130393  0.040058  0.083141  0.067061 -0.009216 -0.016324   \n",
       "2      -0.009086  0.069615  0.034862  0.038376  0.028006 -0.093994 -0.009918   \n",
       "3      -0.037720 -0.066895 -0.001670  0.121809 -0.046596  0.005515  0.122977   \n",
       "4       0.068912 -0.043349  0.059557  0.100227 -0.057617 -0.041477 -0.008450   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "216925  0.107361 -0.002075 -0.014933  0.108297 -0.053955  0.021301  0.122599   \n",
       "216926 -0.091732  0.001933 -0.048096  0.088677 -0.008965 -0.138165 -0.011814   \n",
       "216927  0.017008  0.138156  0.094830  0.047065 -0.004895  0.093838 -0.060109   \n",
       "216928 -0.009044  0.027582  0.027116  0.057123 -0.017256 -0.122869  0.044478   \n",
       "216929  0.081014  0.060028  0.055583  0.102417 -0.012332 -0.052099 -0.021077   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "0      -0.096954  0.073669  0.170628  ... -0.116028  0.010610 -0.076401   \n",
       "1      -0.120292 -0.052221  0.085904  ...  0.007724 -0.054294 -0.065976   \n",
       "2      -0.081107  0.069406  0.085458  ... -0.020455 -0.007708 -0.079356   \n",
       "3      -0.070626  0.034180  0.102086  ... -0.056553 -0.096819 -0.239641   \n",
       "4      -0.088745  0.126736  0.021566  ... -0.074632 -0.128906 -0.114217   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "216925  0.017049  0.013590  0.141591  ... -0.253113  0.040649 -0.240438   \n",
       "216926 -0.078559  0.034383  0.077260  ... -0.054901  0.001689 -0.077101   \n",
       "216927 -0.115553  0.040952  0.088121  ...  0.038896 -0.038656 -0.206906   \n",
       "216928 -0.030525  0.089511  0.046853  ... -0.144798  0.059027 -0.053511   \n",
       "216929 -0.052429  0.106099  0.080048  ... -0.042480 -0.012171 -0.113688   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0       0.055969 -0.106514 -0.060791 -0.023376 -0.097069  0.030815  0.033997  \n",
       "1      -0.066806  0.032055 -0.100475 -0.066428 -0.057850  0.082839  0.022688  \n",
       "2      -0.022810  0.028102 -0.041992 -0.089992 -0.023980  0.044006 -0.089634  \n",
       "3       0.132499  0.092111  0.002973 -0.082136 -0.019723 -0.042411 -0.167899  \n",
       "4       0.062384 -0.047323 -0.034776 -0.146484 -0.012926  0.143745  0.067464  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "216925 -0.003825 -0.082634  0.094910 -0.000905  0.041829  0.086100  0.024526  \n",
       "216926  0.111030  0.054688 -0.004869  0.003432 -0.007151  0.124410 -0.069255  \n",
       "216927  0.078179 -0.065199 -0.045064 -0.102254  0.066569  0.072320  0.055420  \n",
       "216928 -0.043599  0.018205  0.007990 -0.062600 -0.144731  0.088224 -0.081587  \n",
       "216929 -0.022196  0.027520 -0.100634 -0.027883 -0.065681 -0.009949  0.075165  \n",
       "\n",
       "[216930 rows x 300 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(question_vectors.shape)\n",
    "print(type(question_vectors))\n",
    "question_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219383f0",
   "metadata": {},
   "source": [
    "Yay! My husband's gaming PC was able to handle it. Moving on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "843f6fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053513</td>\n",
       "      <td>0.028366</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0.060440</td>\n",
       "      <td>-0.011749</td>\n",
       "      <td>0.018772</td>\n",
       "      <td>0.052729</td>\n",
       "      <td>-0.096954</td>\n",
       "      <td>0.073669</td>\n",
       "      <td>0.170628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>-0.076401</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>-0.106514</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.023376</td>\n",
       "      <td>-0.097069</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.033997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.070263</td>\n",
       "      <td>0.130393</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.083141</td>\n",
       "      <td>0.067061</td>\n",
       "      <td>-0.009216</td>\n",
       "      <td>-0.016324</td>\n",
       "      <td>-0.120292</td>\n",
       "      <td>-0.052221</td>\n",
       "      <td>0.085904</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054294</td>\n",
       "      <td>-0.065976</td>\n",
       "      <td>-0.066806</td>\n",
       "      <td>0.032055</td>\n",
       "      <td>-0.100475</td>\n",
       "      <td>-0.066428</td>\n",
       "      <td>-0.057850</td>\n",
       "      <td>0.082839</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009086</td>\n",
       "      <td>0.069615</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.038376</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>-0.093994</td>\n",
       "      <td>-0.009918</td>\n",
       "      <td>-0.081107</td>\n",
       "      <td>0.069406</td>\n",
       "      <td>0.085458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007708</td>\n",
       "      <td>-0.079356</td>\n",
       "      <td>-0.022810</td>\n",
       "      <td>0.028102</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>-0.089992</td>\n",
       "      <td>-0.023980</td>\n",
       "      <td>0.044006</td>\n",
       "      <td>-0.089634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037720</td>\n",
       "      <td>-0.066895</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>0.121809</td>\n",
       "      <td>-0.046596</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.122977</td>\n",
       "      <td>-0.070626</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.102086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096819</td>\n",
       "      <td>-0.239641</td>\n",
       "      <td>0.132499</td>\n",
       "      <td>0.092111</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>-0.082136</td>\n",
       "      <td>-0.019723</td>\n",
       "      <td>-0.042411</td>\n",
       "      <td>-0.167899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068912</td>\n",
       "      <td>-0.043349</td>\n",
       "      <td>0.059557</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>-0.041477</td>\n",
       "      <td>-0.008450</td>\n",
       "      <td>-0.088745</td>\n",
       "      <td>0.126736</td>\n",
       "      <td>0.021566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128906</td>\n",
       "      <td>-0.114217</td>\n",
       "      <td>0.062384</td>\n",
       "      <td>-0.047323</td>\n",
       "      <td>-0.034776</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>0.143745</td>\n",
       "      <td>0.067464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216925</th>\n",
       "      <td>0.107361</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>-0.014933</td>\n",
       "      <td>0.108297</td>\n",
       "      <td>-0.053955</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.122599</td>\n",
       "      <td>0.017049</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.141591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040649</td>\n",
       "      <td>-0.240438</td>\n",
       "      <td>-0.003825</td>\n",
       "      <td>-0.082634</td>\n",
       "      <td>0.094910</td>\n",
       "      <td>-0.000905</td>\n",
       "      <td>0.041829</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216926</th>\n",
       "      <td>-0.091732</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>0.088677</td>\n",
       "      <td>-0.008965</td>\n",
       "      <td>-0.138165</td>\n",
       "      <td>-0.011814</td>\n",
       "      <td>-0.078559</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.077260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>-0.077101</td>\n",
       "      <td>0.111030</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>-0.004869</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>-0.007151</td>\n",
       "      <td>0.124410</td>\n",
       "      <td>-0.069255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216927</th>\n",
       "      <td>0.017008</td>\n",
       "      <td>0.138156</td>\n",
       "      <td>0.094830</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>0.093838</td>\n",
       "      <td>-0.060109</td>\n",
       "      <td>-0.115553</td>\n",
       "      <td>0.040952</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038656</td>\n",
       "      <td>-0.206906</td>\n",
       "      <td>0.078179</td>\n",
       "      <td>-0.065199</td>\n",
       "      <td>-0.045064</td>\n",
       "      <td>-0.102254</td>\n",
       "      <td>0.066569</td>\n",
       "      <td>0.072320</td>\n",
       "      <td>0.055420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216928</th>\n",
       "      <td>-0.009044</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>0.027116</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>-0.017256</td>\n",
       "      <td>-0.122869</td>\n",
       "      <td>0.044478</td>\n",
       "      <td>-0.030525</td>\n",
       "      <td>0.089511</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059027</td>\n",
       "      <td>-0.053511</td>\n",
       "      <td>-0.043599</td>\n",
       "      <td>0.018205</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>-0.062600</td>\n",
       "      <td>-0.144731</td>\n",
       "      <td>0.088224</td>\n",
       "      <td>-0.081587</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216929</th>\n",
       "      <td>0.081014</td>\n",
       "      <td>0.060028</td>\n",
       "      <td>0.055583</td>\n",
       "      <td>0.102417</td>\n",
       "      <td>-0.012332</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.021077</td>\n",
       "      <td>-0.052429</td>\n",
       "      <td>0.106099</td>\n",
       "      <td>0.080048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012171</td>\n",
       "      <td>-0.113688</td>\n",
       "      <td>-0.022196</td>\n",
       "      <td>0.027520</td>\n",
       "      <td>-0.100634</td>\n",
       "      <td>-0.027883</td>\n",
       "      <td>-0.065681</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216930 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.053513  0.028366 -0.003403  0.060440 -0.011749  0.018772  0.052729   \n",
       "1      -0.070263  0.130393  0.040058  0.083141  0.067061 -0.009216 -0.016324   \n",
       "2      -0.009086  0.069615  0.034862  0.038376  0.028006 -0.093994 -0.009918   \n",
       "3      -0.037720 -0.066895 -0.001670  0.121809 -0.046596  0.005515  0.122977   \n",
       "4       0.068912 -0.043349  0.059557  0.100227 -0.057617 -0.041477 -0.008450   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "216925  0.107361 -0.002075 -0.014933  0.108297 -0.053955  0.021301  0.122599   \n",
       "216926 -0.091732  0.001933 -0.048096  0.088677 -0.008965 -0.138165 -0.011814   \n",
       "216927  0.017008  0.138156  0.094830  0.047065 -0.004895  0.093838 -0.060109   \n",
       "216928 -0.009044  0.027582  0.027116  0.057123 -0.017256 -0.122869  0.044478   \n",
       "216929  0.081014  0.060028  0.055583  0.102417 -0.012332 -0.052099 -0.021077   \n",
       "\n",
       "               7         8         9  ...       291       292       293  \\\n",
       "0      -0.096954  0.073669  0.170628  ...  0.010610 -0.076401  0.055969   \n",
       "1      -0.120292 -0.052221  0.085904  ... -0.054294 -0.065976 -0.066806   \n",
       "2      -0.081107  0.069406  0.085458  ... -0.007708 -0.079356 -0.022810   \n",
       "3      -0.070626  0.034180  0.102086  ... -0.096819 -0.239641  0.132499   \n",
       "4      -0.088745  0.126736  0.021566  ... -0.128906 -0.114217  0.062384   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "216925  0.017049  0.013590  0.141591  ...  0.040649 -0.240438 -0.003825   \n",
       "216926 -0.078559  0.034383  0.077260  ...  0.001689 -0.077101  0.111030   \n",
       "216927 -0.115553  0.040952  0.088121  ... -0.038656 -0.206906  0.078179   \n",
       "216928 -0.030525  0.089511  0.046853  ...  0.059027 -0.053511 -0.043599   \n",
       "216929 -0.052429  0.106099  0.080048  ... -0.012171 -0.113688 -0.022196   \n",
       "\n",
       "             294       295       296       297       298       299  value  \n",
       "0      -0.106514 -0.060791 -0.023376 -0.097069  0.030815  0.033997      0  \n",
       "1       0.032055 -0.100475 -0.066428 -0.057850  0.082839  0.022688      0  \n",
       "2       0.028102 -0.041992 -0.089992 -0.023980  0.044006 -0.089634      0  \n",
       "3       0.092111  0.002973 -0.082136 -0.019723 -0.042411 -0.167899      0  \n",
       "4      -0.047323 -0.034776 -0.146484 -0.012926  0.143745  0.067464      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "216925 -0.082634  0.094910 -0.000905  0.041829  0.086100  0.024526      1  \n",
       "216926  0.054688 -0.004869  0.003432 -0.007151  0.124410 -0.069255      1  \n",
       "216927 -0.065199 -0.045064 -0.102254  0.066569  0.072320  0.055420      1  \n",
       "216928  0.018205  0.007990 -0.062600 -0.144731  0.088224 -0.081587      1  \n",
       "216929  0.027520 -0.100634 -0.027883 -0.065681 -0.009949  0.075165      0  \n",
       "\n",
       "[216930 rows x 301 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_vectors['value'] = df['high_low']\n",
    "question_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544766f2",
   "metadata": {},
   "source": [
    "Got an error in a later step saying that the data contains null values. I guess there were a few questions that didn't contain any words from GoogleNews. Removing all lines with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33d9a5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053513</td>\n",
       "      <td>0.028366</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0.060440</td>\n",
       "      <td>-0.011749</td>\n",
       "      <td>0.018772</td>\n",
       "      <td>0.052729</td>\n",
       "      <td>-0.096954</td>\n",
       "      <td>0.073669</td>\n",
       "      <td>0.170628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>-0.076401</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>-0.106514</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.023376</td>\n",
       "      <td>-0.097069</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.033997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.070263</td>\n",
       "      <td>0.130393</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.083141</td>\n",
       "      <td>0.067061</td>\n",
       "      <td>-0.009216</td>\n",
       "      <td>-0.016324</td>\n",
       "      <td>-0.120292</td>\n",
       "      <td>-0.052221</td>\n",
       "      <td>0.085904</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054294</td>\n",
       "      <td>-0.065976</td>\n",
       "      <td>-0.066806</td>\n",
       "      <td>0.032055</td>\n",
       "      <td>-0.100475</td>\n",
       "      <td>-0.066428</td>\n",
       "      <td>-0.057850</td>\n",
       "      <td>0.082839</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009086</td>\n",
       "      <td>0.069615</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.038376</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>-0.093994</td>\n",
       "      <td>-0.009918</td>\n",
       "      <td>-0.081107</td>\n",
       "      <td>0.069406</td>\n",
       "      <td>0.085458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007708</td>\n",
       "      <td>-0.079356</td>\n",
       "      <td>-0.022810</td>\n",
       "      <td>0.028102</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>-0.089992</td>\n",
       "      <td>-0.023980</td>\n",
       "      <td>0.044006</td>\n",
       "      <td>-0.089634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037720</td>\n",
       "      <td>-0.066895</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>0.121809</td>\n",
       "      <td>-0.046596</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.122977</td>\n",
       "      <td>-0.070626</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.102086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096819</td>\n",
       "      <td>-0.239641</td>\n",
       "      <td>0.132499</td>\n",
       "      <td>0.092111</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>-0.082136</td>\n",
       "      <td>-0.019723</td>\n",
       "      <td>-0.042411</td>\n",
       "      <td>-0.167899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068912</td>\n",
       "      <td>-0.043349</td>\n",
       "      <td>0.059557</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>-0.041477</td>\n",
       "      <td>-0.008450</td>\n",
       "      <td>-0.088745</td>\n",
       "      <td>0.126736</td>\n",
       "      <td>0.021566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128906</td>\n",
       "      <td>-0.114217</td>\n",
       "      <td>0.062384</td>\n",
       "      <td>-0.047323</td>\n",
       "      <td>-0.034776</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>0.143745</td>\n",
       "      <td>0.067464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216925</th>\n",
       "      <td>0.107361</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>-0.014933</td>\n",
       "      <td>0.108297</td>\n",
       "      <td>-0.053955</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.122599</td>\n",
       "      <td>0.017049</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.141591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040649</td>\n",
       "      <td>-0.240438</td>\n",
       "      <td>-0.003825</td>\n",
       "      <td>-0.082634</td>\n",
       "      <td>0.094910</td>\n",
       "      <td>-0.000905</td>\n",
       "      <td>0.041829</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216926</th>\n",
       "      <td>-0.091732</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>0.088677</td>\n",
       "      <td>-0.008965</td>\n",
       "      <td>-0.138165</td>\n",
       "      <td>-0.011814</td>\n",
       "      <td>-0.078559</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.077260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>-0.077101</td>\n",
       "      <td>0.111030</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>-0.004869</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>-0.007151</td>\n",
       "      <td>0.124410</td>\n",
       "      <td>-0.069255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216927</th>\n",
       "      <td>0.017008</td>\n",
       "      <td>0.138156</td>\n",
       "      <td>0.094830</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>0.093838</td>\n",
       "      <td>-0.060109</td>\n",
       "      <td>-0.115553</td>\n",
       "      <td>0.040952</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038656</td>\n",
       "      <td>-0.206906</td>\n",
       "      <td>0.078179</td>\n",
       "      <td>-0.065199</td>\n",
       "      <td>-0.045064</td>\n",
       "      <td>-0.102254</td>\n",
       "      <td>0.066569</td>\n",
       "      <td>0.072320</td>\n",
       "      <td>0.055420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216928</th>\n",
       "      <td>-0.009044</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>0.027116</td>\n",
       "      <td>0.057123</td>\n",
       "      <td>-0.017256</td>\n",
       "      <td>-0.122869</td>\n",
       "      <td>0.044478</td>\n",
       "      <td>-0.030525</td>\n",
       "      <td>0.089511</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059027</td>\n",
       "      <td>-0.053511</td>\n",
       "      <td>-0.043599</td>\n",
       "      <td>0.018205</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>-0.062600</td>\n",
       "      <td>-0.144731</td>\n",
       "      <td>0.088224</td>\n",
       "      <td>-0.081587</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216929</th>\n",
       "      <td>0.081014</td>\n",
       "      <td>0.060028</td>\n",
       "      <td>0.055583</td>\n",
       "      <td>0.102417</td>\n",
       "      <td>-0.012332</td>\n",
       "      <td>-0.052099</td>\n",
       "      <td>-0.021077</td>\n",
       "      <td>-0.052429</td>\n",
       "      <td>0.106099</td>\n",
       "      <td>0.080048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012171</td>\n",
       "      <td>-0.113688</td>\n",
       "      <td>-0.022196</td>\n",
       "      <td>0.027520</td>\n",
       "      <td>-0.100634</td>\n",
       "      <td>-0.027883</td>\n",
       "      <td>-0.065681</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215195 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.053513  0.028366 -0.003403  0.060440 -0.011749  0.018772  0.052729   \n",
       "1      -0.070263  0.130393  0.040058  0.083141  0.067061 -0.009216 -0.016324   \n",
       "2      -0.009086  0.069615  0.034862  0.038376  0.028006 -0.093994 -0.009918   \n",
       "3      -0.037720 -0.066895 -0.001670  0.121809 -0.046596  0.005515  0.122977   \n",
       "4       0.068912 -0.043349  0.059557  0.100227 -0.057617 -0.041477 -0.008450   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "216925  0.107361 -0.002075 -0.014933  0.108297 -0.053955  0.021301  0.122599   \n",
       "216926 -0.091732  0.001933 -0.048096  0.088677 -0.008965 -0.138165 -0.011814   \n",
       "216927  0.017008  0.138156  0.094830  0.047065 -0.004895  0.093838 -0.060109   \n",
       "216928 -0.009044  0.027582  0.027116  0.057123 -0.017256 -0.122869  0.044478   \n",
       "216929  0.081014  0.060028  0.055583  0.102417 -0.012332 -0.052099 -0.021077   \n",
       "\n",
       "               7         8         9  ...       291       292       293  \\\n",
       "0      -0.096954  0.073669  0.170628  ...  0.010610 -0.076401  0.055969   \n",
       "1      -0.120292 -0.052221  0.085904  ... -0.054294 -0.065976 -0.066806   \n",
       "2      -0.081107  0.069406  0.085458  ... -0.007708 -0.079356 -0.022810   \n",
       "3      -0.070626  0.034180  0.102086  ... -0.096819 -0.239641  0.132499   \n",
       "4      -0.088745  0.126736  0.021566  ... -0.128906 -0.114217  0.062384   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "216925  0.017049  0.013590  0.141591  ...  0.040649 -0.240438 -0.003825   \n",
       "216926 -0.078559  0.034383  0.077260  ...  0.001689 -0.077101  0.111030   \n",
       "216927 -0.115553  0.040952  0.088121  ... -0.038656 -0.206906  0.078179   \n",
       "216928 -0.030525  0.089511  0.046853  ...  0.059027 -0.053511 -0.043599   \n",
       "216929 -0.052429  0.106099  0.080048  ... -0.012171 -0.113688 -0.022196   \n",
       "\n",
       "             294       295       296       297       298       299  value  \n",
       "0      -0.106514 -0.060791 -0.023376 -0.097069  0.030815  0.033997      0  \n",
       "1       0.032055 -0.100475 -0.066428 -0.057850  0.082839  0.022688      0  \n",
       "2       0.028102 -0.041992 -0.089992 -0.023980  0.044006 -0.089634      0  \n",
       "3       0.092111  0.002973 -0.082136 -0.019723 -0.042411 -0.167899      0  \n",
       "4      -0.047323 -0.034776 -0.146484 -0.012926  0.143745  0.067464      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "216925 -0.082634  0.094910 -0.000905  0.041829  0.086100  0.024526      1  \n",
       "216926  0.054688 -0.004869  0.003432 -0.007151  0.124410 -0.069255      1  \n",
       "216927 -0.065199 -0.045064 -0.102254  0.066569  0.072320  0.055420      1  \n",
       "216928  0.018205  0.007990 -0.062600 -0.144731  0.088224 -0.081587      1  \n",
       "216929  0.027520 -0.100634 -0.027883 -0.065681 -0.009949  0.075165      0  \n",
       "\n",
       "[215195 rows x 301 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_vectors = question_vectors.dropna()\n",
    "question_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5f2b1",
   "metadata": {},
   "source": [
    "Now to split the data into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63de5ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((172156, 300), (43039, 300), (172156,), (43039,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(question_vectors.drop('value', axis=1),\n",
    "                                                   question_vectors['value'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe992d5",
   "metadata": {},
   "source": [
    "Multiple examples of similar code I found used AdaBoostClassifier from scikit-learn for the next part, so gonna give that a try and use here instead of Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35514952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "AdaBoost = AdaBoostClassifier(random_state = 1)\n",
    "AdaBoost.fit(train_x, train_y)\n",
    "test_pred = AdaBoost.predict(test_x)\n",
    "\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(test_y, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6cfeb",
   "metadata": {},
   "source": [
    "Wow! Not great. I really expected it to be higher. Wonder if there's anything that could be done to improve it...\n",
    "\n",
    "Found a neat flowchart about different selecting the right machince learning algorithm based off the data you have and what you're trying to do with it.\n",
    "\n",
    "https://scikit-learn.org/stable/_static/ml_map.png\n",
    "\n",
    "It suggests that SGD Classifier is proabably best for us. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5463bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "clf.fit(train_x, train_y)\n",
    "y_pred = clf.predict(test_x)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(test_y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfb00f",
   "metadata": {},
   "source": [
    "Well, that wan't great, either. Ok, supposedly scaling is important when using SGD Classifier. I'll try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02ea4daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)\n",
    "train_x = scaler.transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "clf.fit(train_x, train_y)\n",
    "y_pred = clf.predict(test_x)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(test_y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a06a01",
   "metadata": {},
   "source": [
    "not better. maybe go back to Naive_Bayes??? but with the vectors from google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "574a3275",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[0;32m      3\u001b[0m naive_bayes \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mnaive_bayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m naive_bayes\u001b[38;5;241m.\u001b[39mpredict(test_x)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy_score(test_y, y_pred)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sklearn\\lib\\site-packages\\sklearn\\naive_bayes.py:690\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    688\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sklearn\\lib\\site-packages\\sklearn\\naive_bayes.py:863\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sklearn\\lib\\site-packages\\sklearn\\utils\\validation.py:1249\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1246\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(train_x, train_y)\n",
    "y_pred = naive_bayes.predict(test_x)\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(test_y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c582940f",
   "metadata": {},
   "source": [
    "I guess these types of vectors are not meant to be used with Naive Bayes. I'll try one or two more form the list before calling it quits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bcefe2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(train_x, train_y)\n",
    "y_pred = classifier.predict(test_x)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(test_y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ba13f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(train_x, train_y)\n",
    "y_pred = classifier.predict(test_x)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(test_y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cdf14",
   "metadata": {},
   "source": [
    "Don't know what else to try to get that number higher. I give up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5618f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sklearn)",
   "language": "python",
   "name": "sklearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
